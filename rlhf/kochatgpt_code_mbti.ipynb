{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLUYAx-jSMVQ"
      },
      "source": [
        "# Make ChatGPT-replica!!\n",
        "\n",
        "\"학습을 시작하기 전에\" 파트를 반드시 실행시켜 주세요.\n",
        "\n",
        "각 파트 사이사이 마다 raise Exception 구문이 있기 때문에, 한 파트만 학습하고 싶은 경우에도 그냥 이후 셀 실행 하면 됩니다.\n",
        "\n",
        "한 번에 1~3번을 모두 수행하고 싶은 경우, 중간중간에 있는 에러 구문을 제거해주시기 바랍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습을 시작하기 전에"
      ],
      "metadata": {
        "id": "xDB-y_f4RoYQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5CDIjagvjy"
      },
      "source": [
        "### Google Drive에 모델 저장하기\n",
        "한 번 학습한 모델을 재활용하기 위해 google drive에 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BJFXDikuOLC",
        "outputId": "d2980d7a-ef3e-46b5-d122-df7b83fe46d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bKSqYythYwF"
      },
      "source": [
        "### 상수, 경로 설정 부분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPqW4G2WhcAR"
      },
      "outputs": [],
      "source": [
        "# Pretrained model 이름(hugging face) or 체크포인트 저장 경로\n",
        "SFT_MODEL_NAME = './drive/MyDrive/Models/final_SFT_mbtiF_kakao'\n",
        "\n",
        "SFT_DATASET_PATH = './drive/MyDrive/Models/kakao_cleaned.jsonl'\n",
        "SFT_MODEL_PATH = './drive/MyDrive/Models/final_SFT_mbtiF_kakao2'\n",
        "SFT_MODEL_OUTPUT_PATH = './drive/MyDrive/Models/final_SFT_mbtiF_kakao2'\n",
        "# SFT 모델을 학습시킬 때 샘플링 여부. \n",
        "# 샘플링하지 않고 모든 데이터를 학습시킬 경우 0으로 두기\n",
        "SFT_SAMPLE_DATA_SIZE = 0\n",
        "\n",
        "RM_DATASET_PATH = './drive/MyDrive/Models/kakao_light_rm.jsonl'\n",
        "RM_MODEL_PATH = './drive/MyDrive/Models/output_2_RM_kakao'\n",
        "RM_MODEL_OUTPUT_PATH = './drive/MyDrive/Models/output_2_RM_kakao'\n",
        "\n",
        "PPO_DATASET_PATH = './drive/MyDrive/Models/kakao_ppo.jsonl'\n",
        "PPO_MODEL_PATH = './drive/MyDrive/Models/output_3_PPO'\n",
        "PPO_MODEL_OUTPUT_PATH = './drive/MyDrive/Models/output_3_PPO'\n",
        "\n",
        "# Inference 시 질문 목록\n",
        "PROMPT_LIST = [\n",
        "  '나 기분 좋아서 옷 샀어',\n",
        "\t'나 오늘 좋은 일 있어서 신발 샀어',\n",
        "\t'나 기분 안 좋아서 쇼핑다녀왔어',\n",
        "\t'너 기분 안 좋아보여서 아이스크림 사왔어',\n",
        "\t'나 돈 모아서 컴퓨터 샀어',\n",
        "\t'나 학교 가는 길에 교통사고 났어',\n",
        "\t'나 기분 안 좋아서 머리 잘랐어',\n",
        "\t'요즘 취업 준비하느라 힘들어',\n",
        "\t'오늘 늦게일어나서 대충 머리감고 나왔어',\n",
        "\t'걔가 너 싫어한대',\n",
        "\t'나 배탈 난 것 같아',\n",
        "\t'나 감기 걸린 것 같아'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import modules"
      ],
      "metadata": {
        "id": "w-w-RwdbTFe9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIg6pniqSMVU",
        "outputId": "e3d72b31-a30e-4cc9-f9ef-1739902bfb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.13.1+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1977.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m909.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu116) (4.5.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu116\n",
            "Torch version:1.13.1+cu116\n",
            "cuda version: 11.6\n",
            "cudnn version:8302\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colossalai==0.2.7\n",
            "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
            "Collecting pre-commit (from colossalai==0.2.7)\n",
            "  Downloading pre_commit-3.3.2-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
            "Collecting fabric (from colossalai==0.2.7)\n",
            "  Downloading fabric-3.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contexttimer (from colossalai==0.2.7)\n",
            "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ninja (from colossalai==0.2.7)\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.13.1+cu116)\n",
            "Collecting invoke>=2.0 (from fabric->colossalai==0.2.7)\n",
            "  Downloading invoke-2.1.2-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m364.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai==0.2.7)\n",
            "  Downloading paramiko-3.1.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cfgv>=2.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
            "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
            "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.12.0)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
            "Building wheels for collected packages: colossalai, contexttimer\n",
            "  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896479 sha256=76ab5638c43d805af2582bb7169ddff46cf4343589fad131682c821ff291ed97\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/85/25/32a3af943ea5ca261b1b51dae74a4629599ce1bc6fe58dbbfc\n",
            "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5803 sha256=8f58b5db5bbffdf6f5c1a0f364b41d4ca8e29d40294a126f3df8178cdc201044\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n",
            "Successfully built colossalai contexttimer\n",
            "Installing collected packages: ninja, distlib, contexttimer, virtualenv, nodeenv, invoke, identify, cfgv, bcrypt, pynacl, pre-commit, paramiko, fabric, colossalai\n",
            "Successfully installed bcrypt-4.0.1 cfgv-3.3.1 colossalai-0.2.7 contexttimer-0.3.3 distlib-0.3.6 fabric-3.0.1 identify-2.5.24 invoke-2.1.2 ninja-1.11.1 nodeenv-1.8.0 paramiko-3.1.0 pre-commit-3.3.2 pynacl-1.5.0 virtualenv-20.23.0\n",
            "Cloning into 'KoChatGPT'...\n",
            "remote: Enumerating objects: 240, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 240 (delta 63), reused 84 (delta 45), pack-reused 129\u001b[K\n",
            "Receiving objects: 100% (240/240), 11.57 MiB | 34.54 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.113\n",
            "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.113)\n",
            "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.113)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, mypy-extensions, marshmallow, typing-inspect, marshmallow-enum, dataclasses-json, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "Successfully installed SQLAlchemy-1.4.48 dataclasses-json-0.5.7 langchain-0.0.113 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 typing-inspect-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.12.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting loralib\n",
            "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
            "Installing collected packages: loralib\n",
            "Successfully installed loralib-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.113)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.4.48)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "## setup(1min)\n",
        "# torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
        "!pip uninstall torch -y\n",
        "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__))\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))\n",
        "\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
        "\n",
        "\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "\n",
        "# setup data\n",
        "!git clone https://github.com/airobotlab/KoChatGPT\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1\n",
        "!pip install transformers==4.28.0\n",
        "!pip install tqdm\n",
        "!pip install datasets\n",
        "!pip install loralib\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코랩 안꺼지게 하기\n",
        "\n",
        "function ClickConnect() {\n",
        "    var buttons = document.querySelectorAll(\"colab-dialog.yes-no-dialog paper-button#cancel\"); \n",
        "    buttons.forEach(function(btn) { \n",
        "        btn.click(); \n",
        "    }); \n",
        "    console.log(\"1분마다 자동 재연결\"); \n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click(); \n",
        "} \n",
        "setInterval(ClickConnect,1000*60);\n",
        "\n",
        "function CleanCurrentOutput(){ \n",
        "    var btn = document.querySelector(\".output-icon.clear_outputs_enabled.output-icon-selected[title$='현재 실행 중...'] iron-icon[command=clear-focused-or-selected-outputs]\"); \n",
        "    if(btn) { console.log(\"30분마다 출력 지우기\");\n",
        "     btn.click(); \n",
        "    } \n",
        "} \n",
        "setInterval(CleanCurrentOutput,1000*60*30);"
      ],
      "metadata": {
        "id": "-yI02vwC50b0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Supervised Fine Tuning"
      ],
      "metadata": {
        "id": "mXO7hOuFwvaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "PfLmVk-OV2PR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51S4uJpbSMVV"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "from copy import deepcopy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyperparameter 설정"
      ],
      "metadata": {
        "id": "Utr3zHndV5di"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4OzdlU9SMVV",
        "outputId": "4d526b41-1363-45fa-9ca2-08ecf7f63d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_path_1_SFT='./drive/MyDrive/Models/kakao_cleaned.jsonl', model_name='./drive/MyDrive/Models/final_SFT_mbtiF_only', max_epochs=1, train_batch_size=8, output_dir='./drive/MyDrive/Models/final_SFT_mbtiF_kakao', save_steps=5000)\n"
          ]
        }
      ],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_1_SFT', type=str, default=SFT_DATASET_PATH)\n",
        "parser.add_argument('--model_name', type=str, default='gpt2')\n",
        "parser.add_argument('--max_epochs', type=int, default=5)\n",
        "parser.add_argument('--train_batch_size', type=int, default=16)\n",
        "parser.add_argument('--output_dir', type=str, default=SFT_MODEL_OUTPUT_PATH)\n",
        "parser.add_argument('--save_steps', type=int, default=5000)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.model_name = SFT_MODEL_NAME  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
        "\n",
        "args.max_epochs = 1\n",
        "\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXT_zkuwSMVX"
      },
      "outputs": [],
      "source": [
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(명령어):\\n{query}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"### Instruction(명령어):\\n{query}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "afb10a8b1b164d68949bf7dbf1f6b7b4",
            "66d4fb49c90e4447a5b4a7c1861f20bd",
            "38787e488ec9498baf13f66849b74d26",
            "bf3d64f9ed8445f296a789d28c0e773d",
            "9f46e8f58e2e40388dd3c2ab87f9a76d",
            "654513d28bbd438c8af21a3b717de678",
            "d014641320ea47b581a2b7d01964b5f6",
            "c7a0ed9b3af64c7a9052ea0e37682aad",
            "65ba40ee84524b1e90102568cb2320f5",
            "433380c5b47e4b6289492d5b4ff191d2",
            "f3a1da3384934e2bbd0de5c6361b908a",
            "2183dc035b3e49f68be75b3013e445fe",
            "f976dc34daf34b63b238cb8d85ceba5a",
            "a98a919987c743c6844fe6b5f9deaafb",
            "b4f2e69be88a43a293b276539fedbc7e",
            "879062b5d9d147789d142fe09107ecf5",
            "c0d452703cd14bec875d30c97163031f",
            "bae2ff3bff2448548a95af3b142460bb",
            "3059cbdef1564e4a8f99d269c93db85a",
            "06248e099d274a1fb2967ab38434161c",
            "77b89f1869bc4481b731e302481a9fa8",
            "020c7a7ef00c42fdb15bd6c2c8481c66"
          ]
        },
        "id": "_a_EbaW8SMVX",
        "outputId": "4722723b-ff5e-4337-df54-c63f1b899fde"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afb10a8b1b164d68949bf7dbf1f6b7b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2183dc035b3e49f68be75b3013e445fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=64, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
          ]
        }
      ],
      "source": [
        "## 모델 준비\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=64,    \n",
        ")\n",
        "tokenizer.add_special_tokens(\n",
        "    {\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    }\n",
        ")    \n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"model name : {args.model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ8DaAQc9Hbv",
        "outputId": "5c882218-bb7a-4510-a154-f36931e0d089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model name : ./drive/MyDrive/Models/final_SFT_mbtiF_only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E6u3j_YSMVX",
        "outputId": "588a7023-368b-4fe0-956d-2bd9bb0844f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Loading data done!!: 1119322\n"
          ]
        }
      ],
      "source": [
        "## prepare data\n",
        "from typing import Optional, Dict, Sequence\n",
        "import random\n",
        "import math\n",
        "    \n",
        "class SFT_dataset(Dataset):\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        \n",
        "        ## format\n",
        "        pattern_instruction = 'query'  # instruction\n",
        "        pattern_input = 'input'  # 내 데이터엔 input이 없다\n",
        "        pattern_output = 'response'  # output\n",
        "\n",
        "        ############################################################\n",
        "        ## load dataset\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = []\n",
        "            if SFT_SAMPLE_DATA_SIZE and SFT_SAMPLE_DATA_SIZE > 0:\n",
        "                random.seed(42)\n",
        "                list_data_dict = random.sample(json.load(json_file), SFT_SAMPLE_DATA_SIZE)\n",
        "            else:\n",
        "                list_data_dict = json.load(json_file)\n",
        "    \n",
        "        ############################################################\n",
        "        ## 데이터셋 만들기, source와 target\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
        "\n",
        "        # 입력\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            if example.get(pattern_input, \"\") != \"\":\n",
        "                tmp = prompt_input.format_map(example)\n",
        "            else:\n",
        "                tmp = prompt_no_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        # 출력\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "\n",
        "        if verbose:\n",
        "            idx = 0\n",
        "            print((sources[idx]))\n",
        "            print((targets[idx]))\n",
        "            print(\"Tokenizing inputs... This may take some time...\")\n",
        "\n",
        "        ############################################################\n",
        "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        # source data tokenized\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "\n",
        "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
        "\n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
        "        \n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
        "        \n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        \"\"\"Tokenize a list of strings.\"\"\"\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "    \n",
        "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
        "eval_dataset  = None\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "B8Mu74ggWsqy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uuh3PgkLSMVY",
        "outputId": "6af0f7c3-af96-430a-c87d-92520815c8c4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='106196' max='139916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [106196/139916 17:50 < 1:37:07, 5.79 it/s, Epoch 0.04/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100500</td>\n",
              "      <td>3.811800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101000</td>\n",
              "      <td>3.804100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101500</td>\n",
              "      <td>3.761700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102000</td>\n",
              "      <td>3.776500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102500</td>\n",
              "      <td>3.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103000</td>\n",
              "      <td>3.792300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103500</td>\n",
              "      <td>3.779400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104000</td>\n",
              "      <td>3.801600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104500</td>\n",
              "      <td>3.774500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105000</td>\n",
              "      <td>3.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105500</td>\n",
              "      <td>3.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106000</td>\n",
              "      <td>3.728200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='139713' max='139916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [139713/139916 1:53:55 < 00:34, 5.81 it/s, Epoch 0.28/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100500</td>\n",
              "      <td>3.811800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101000</td>\n",
              "      <td>3.804100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101500</td>\n",
              "      <td>3.761700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102000</td>\n",
              "      <td>3.776500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102500</td>\n",
              "      <td>3.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103000</td>\n",
              "      <td>3.792300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103500</td>\n",
              "      <td>3.779400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104000</td>\n",
              "      <td>3.801600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104500</td>\n",
              "      <td>3.774500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105000</td>\n",
              "      <td>3.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105500</td>\n",
              "      <td>3.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106000</td>\n",
              "      <td>3.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106500</td>\n",
              "      <td>3.762900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107000</td>\n",
              "      <td>3.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107500</td>\n",
              "      <td>3.733300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108000</td>\n",
              "      <td>3.721800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108500</td>\n",
              "      <td>3.725400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109000</td>\n",
              "      <td>3.739400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109500</td>\n",
              "      <td>3.733300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110000</td>\n",
              "      <td>3.708300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110500</td>\n",
              "      <td>3.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111000</td>\n",
              "      <td>3.692500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111500</td>\n",
              "      <td>3.729800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112000</td>\n",
              "      <td>3.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112500</td>\n",
              "      <td>3.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113000</td>\n",
              "      <td>3.684800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113500</td>\n",
              "      <td>3.711500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114000</td>\n",
              "      <td>3.728400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114500</td>\n",
              "      <td>3.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115000</td>\n",
              "      <td>3.706400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115500</td>\n",
              "      <td>3.678400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116000</td>\n",
              "      <td>3.711000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116500</td>\n",
              "      <td>3.711300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117000</td>\n",
              "      <td>3.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117500</td>\n",
              "      <td>3.673000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118000</td>\n",
              "      <td>3.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118500</td>\n",
              "      <td>3.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119000</td>\n",
              "      <td>3.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119500</td>\n",
              "      <td>3.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120000</td>\n",
              "      <td>3.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120500</td>\n",
              "      <td>3.680300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121000</td>\n",
              "      <td>3.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121500</td>\n",
              "      <td>3.664100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122000</td>\n",
              "      <td>3.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122500</td>\n",
              "      <td>3.653600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123000</td>\n",
              "      <td>3.661200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123500</td>\n",
              "      <td>3.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124000</td>\n",
              "      <td>3.656900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124500</td>\n",
              "      <td>3.657900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125000</td>\n",
              "      <td>3.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125500</td>\n",
              "      <td>3.601400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126000</td>\n",
              "      <td>3.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126500</td>\n",
              "      <td>3.644900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127000</td>\n",
              "      <td>3.644800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127500</td>\n",
              "      <td>3.639300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128000</td>\n",
              "      <td>3.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128500</td>\n",
              "      <td>3.648900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129000</td>\n",
              "      <td>3.618800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129500</td>\n",
              "      <td>3.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130000</td>\n",
              "      <td>3.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130500</td>\n",
              "      <td>3.653000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131000</td>\n",
              "      <td>3.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131500</td>\n",
              "      <td>3.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132000</td>\n",
              "      <td>3.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132500</td>\n",
              "      <td>3.657000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133000</td>\n",
              "      <td>3.619800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133500</td>\n",
              "      <td>3.622200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134000</td>\n",
              "      <td>3.610900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134500</td>\n",
              "      <td>3.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135000</td>\n",
              "      <td>3.624900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135500</td>\n",
              "      <td>3.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136000</td>\n",
              "      <td>3.636500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136500</td>\n",
              "      <td>3.593700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137000</td>\n",
              "      <td>3.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137500</td>\n",
              "      <td>3.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138000</td>\n",
              "      <td>3.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138500</td>\n",
              "      <td>3.608500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139000</td>\n",
              "      <td>3.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139500</td>\n",
              "      <td>3.609200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## 학습 (10min)\n",
        "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=SFT_MODEL_OUTPUT_PATH, #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs = args.max_epochs, # number of training epochs\n",
        "    per_device_train_batch_size = args.train_batch_size, # batch size for training\n",
        "    per_device_eval_batch_size = args.train_batch_size,  # batch size for evaluation\n",
        "    save_steps = args.save_steps, # after # steps model is saved \n",
        "    warmup_steps = 5,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    resume_from_checkpoint=True,\n",
        "    learning_rate = 5e-5,\n",
        "    ignore_data_skip = True\n",
        "    )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=True)\n",
        "trainer.save_state()\n",
        "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)\n",
        "print(\"SFT Model Train done!!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추론"
      ],
      "metadata": {
        "id": "uPRJF2EEWufC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7CFw58BSMVZ"
      },
      "outputs": [],
      "source": [
        "## 추론 테스트\n",
        "generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375, # \\n\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "list_prompt = PROMPT_LIST\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'query' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print(('#'*70))\n",
        "    print(('completion: %s'%(result[0]['generated_text'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7EindF6SMVZ"
      },
      "outputs": [],
      "source": [
        "raise Exception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU4djUZ_7tit"
      },
      "source": [
        "# 2. RM: 좋은 글 채점기 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import (1에서 import하지 않은 상황)"
      ],
      "metadata": {
        "id": "iEfSfebZW8pP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KphqHc6lgu38"
      },
      "outputs": [],
      "source": [
        "# ## setup(1min)\n",
        "# # torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
        "# !pip uninstall torch -y\n",
        "# !pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "\n",
        "# import torch\n",
        "\n",
        "# print(\"Torch version:{}\".format(torch.__version__))\n",
        "# print(\"cuda version: {}\".format(torch.version.cuda))\n",
        "# print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
        "\n",
        "# # for ColossalAI\n",
        "# !pip install colossalai==0.2.7\n",
        "\n",
        "# # setup data\n",
        "# !git clone https://github.com/airobotlab/KoChatGPT\n",
        "# !mv KoChatGPT/data_kochatgpt .\n",
        "# !mv KoChatGPT/img .\n",
        "\n",
        "# %cd KoChatGPT/colossalai_ChatGPT_230319/\n",
        "# !pip install .\n",
        "# %cd ../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiuVfmvb7tTD"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import argparse\n",
        "\n",
        "import loralib as lora\n",
        "import torch\n",
        "torch.cuda.empty_cache() \n",
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMRM\n",
        "from chatgpt.models.gpt import GPTRM\n",
        "from chatgpt.models.opt import OPTRM\n",
        "from chatgpt.trainer import RewardModelTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from datasets import load_dataset\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RM hyperparameter 설정"
      ],
      "metadata": {
        "id": "OK1iBS05XXvJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNEweAK77tNT"
      },
      "outputs": [],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--output_dir', type=str, default=RM_MODEL_OUTPUT_PATH)\n",
        "parser.add_argument('--data_path_2_RM', type=str, default=RM_DATASET_PATH, help='https://huggingface.co/datasets/fka/awesome-chatgpt-prompts/blob/main/prompts.csv')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
        "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
        "parser.add_argument('--max_epochs', type=int, default=10)\n",
        "parser.add_argument('--batch_size', type=int, default=4)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_len', type=int, default=64)  # wygo 추가\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.max_epochs = 5\n",
        "args.pretrain = SFT_MODEL_NAME  # pretrained 모델 가져오기\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['RANK'] = '0'\n",
        "os.environ['LOCAL_RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '2'\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '42043'"
      ],
      "metadata": {
        "id": "mwO58p61-uuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtwPLUIq7tIb"
      },
      "outputs": [],
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ96VsvwnHG9"
      },
      "outputs": [],
      "source": [
        "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
        "from typing import Optional\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "# from ..base import RewardModel\n",
        "from chatgpt.models.base import RewardModel\n",
        "\n",
        "\n",
        "class GPTRM_custom(RewardModel):\n",
        "    \"\"\"\n",
        "    GPT Reward model.\n",
        "    Args:\n",
        "        pretrained (str): Pretrained model name or path.\n",
        "        config (GPT2Config): Model config.\n",
        "        checkpoint (bool): Enable gradient checkpointing.\n",
        "        lora_rank (int): Rank of the low-rank approximation.\n",
        "        lora_train_bias (str): LoRA bias training mode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "        if pretrained is not None:\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        \n",
        "        # model = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
        "\n",
        "        # 추가, 230421    \n",
        "        if pretrained is not None:\n",
        "            self.model = model\n",
        "            self.pretrained = pretrained\n",
        "        \n",
        "    # 추가, 230421, config.json을 생성하기 위해 추가\n",
        "    def save_pretrained(self, dir):\n",
        "        if self.pretrained is not None:\n",
        "            self.model.save_pretrained(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee3we4uB8EdK"
      },
      "outputs": [],
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    # load pretrained gpt2    \n",
        "    if args.model == 'gpt2':\n",
        "#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "    \n",
        "    elif args.model == 'opt':\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "    \n",
        "    \n",
        "    # model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD94Zd-b7tDL"
      },
      "outputs": [],
      "source": [
        "# make ranking data to chosen, rejetced data\n",
        "with open(args.data_path_2_RM, \"r\", encoding='utf-8') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    if args.verbose:\n",
        "        print('## data check ##')\n",
        "        print((list_data_dict[0]))\n",
        "        \n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    # data 2) 0 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # data 1) 1 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "    \n",
        "    \n",
        "    \n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[40])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWzG_w3d7s9h"
      },
      "outputs": [],
      "source": [
        "# prepare for data and dataset\n",
        "import random\n",
        "import math\n",
        "\n",
        "random.seed(230319)\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[45])\n",
        "\n",
        "train_data_len = math.floor(len(total_data_ranking2chosen) * 0.8)\n",
        "train_data = total_data_ranking2chosen[:train_data_len]\n",
        "eval_data = total_data_ranking2chosen[train_data_len:] \n",
        "\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "print(f'train set : {len(train_data)}, eval_set : {len(eval_data)}')\n",
        "\n",
        "# check\n",
        "idx = 10\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQI5a1hm7s3L"
      },
      "outputs": [],
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
        "else:\n",
        "    optim = Adam(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97mbryrTgXJ0"
      },
      "outputs": [],
      "source": [
        "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
        "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=strategy,\n",
        "                             optim=optim,\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             max_epochs=args.max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE6HOKdmgY6X"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpOYBhbH8HLh"
      },
      "outputs": [],
      "source": [
        "# train!!\n",
        "trainer.fit(use_lora=args.lora_rank)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(optim,\n",
        "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)\n",
        "\n",
        "model.save_pretrained(args.output_dir)  # config.json 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추론"
      ],
      "metadata": {
        "id": "LN8diXLnYTom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaGv2_qUsRrb"
      },
      "outputs": [],
      "source": [
        "# 보상모델 체크\n",
        "def inference_RM(input_text='인공지능은 인공지능 입니다'):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    output = model(input_ids)\n",
        "    output_reward = output.cpu().detach().numpy()[0]\n",
        "    \n",
        "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
        "    \n",
        "    return output_reward\n",
        "\n",
        "\n",
        "# input_text = '한국은 대한민국 입니다'\n",
        "input_text = '인공지능은 인공지능 입니다'\n",
        "\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# Test\n",
        "inference_RM(input_text='어떤 신발을 샀어?')\n",
        "inference_RM(input_text='샀다고 신발을?')\n",
        "inference_RM(input_text='어육 제육 키키')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_nNydSKAtOu"
      },
      "outputs": [],
      "source": [
        "raise Exception"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYQV4ca3Ytna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqkr7S_-pO9m"
      },
      "source": [
        "#3. PPO 사람의 피드백을 반영하여 학습\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import(1, 2에서 하지 않은 경우)"
      ],
      "metadata": {
        "id": "-EKBo-gqYqwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fo6KWWcpdxh"
      },
      "outputs": [],
      "source": [
        "## setup(1min)\n",
        "# torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
        "!pip uninstall torch -y\n",
        "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__))\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
        "\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "\n",
        "# setup data\n",
        "!git clone https://github.com/airobotlab/KoChatGPT\n",
        "!mv KoChatGPT/data_kochatgpt .\n",
        "!mv KoChatGPT/img .\n",
        "\n",
        "\n",
        "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
        "!pip install .\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Ytmwj-diQ6"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.models.opt import OPTActor, OPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "## wy 추가\n",
        "import json\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "## clossalAI error 해결\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['LOCAL_RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '2'\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '42043'\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter 설정"
      ],
      "metadata": {
        "id": "E0najiqgYx_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctd5oDVCdiJJ"
      },
      "outputs": [],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_3_PPO', type=str, default=PPO_DATASET_PATH)\n",
        "parser.add_argument('--output_dir', type=str, default=PPO_MODEL_OUTPUT_PATH)\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--num_episodes', type=int, default=50)\n",
        "parser.add_argument('--max_timesteps', type=int, default=80)\n",
        "parser.add_argument('--steps', type=int, default=80)\n",
        "parser.add_argument('--update_timesteps', type=int, default=2)\n",
        "parser.add_argument('--max_epochs', type=int, default=2)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_length', type=int, default=64)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.output_dir = PPO_MODEL_OUTPUT_PATH\n",
        "args.pretrain = SFT_MODEL_NAME  # pretrained 모델 가져오기\n",
        "\n",
        "\n",
        "## 이곳 수정!!\n",
        "args.pretrain_actor = SFT_MODEL_PATH  # SFT 모델 가져오기\n",
        "args.pretrain_critic = RM_MODEL_PATH  # RM 모델 가져오기\n",
        "# args.pretrain_actor = args.pretrain\n",
        "# args.pretrain_critic = args.pretrain\n",
        "\n",
        "args.num_episodes = 15\n",
        "args.max_epochs   = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5YEhGiIdiBQ"
      },
      "outputs": [],
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZJ4oN9BdnL6"
      },
      "outputs": [],
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "    tokenizer.add_special_tokens(\n",
        "        {\n",
        "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "        }\n",
        "    )    \n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "    initial_model = deepcopy(actor)\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OnJ76tgdnAr"
      },
      "outputs": [],
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
        "else:\n",
        "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CPTGw5TeW9p"
      },
      "outputs": [],
      "source": [
        "# setting the models\n",
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM3Tm4Lhdh5U"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
        "    return {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "print(len(list_prompt))\n",
        "print('\\n\\n\\n')\n",
        "print(tokenize_fn('I want you to act as a linux terminal.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "_xQRhhJwY1vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAS01J_iedt5"
      },
      "outputs": [],
      "source": [
        "# configure trainer\n",
        "trainer = PPOTrainer(strategy,\n",
        "                     actor,\n",
        "                     critic,\n",
        "                     reward_model,\n",
        "                     initial_model,\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=args.max_epochs,\n",
        "                     train_batch_size=args.train_batch_size,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "## train!\n",
        "trainer.fit(list_prompt,  # 입력 prompt\n",
        "            num_episodes=args.num_episodes,\n",
        "            max_timesteps=args.max_timesteps,\n",
        "            update_timesteps=args.update_timesteps)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(actor_optim,\n",
        "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추론"
      ],
      "metadata": {
        "id": "51F0fXFgY4Di"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjmSYfy2xBi5"
      },
      "outputs": [],
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print('#' * 70)\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "list_prompt = PROMPT_LIST\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "    output = generation(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY84CFCvA6N-"
      },
      "outputs": [],
      "source": [
        "raise Exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvp1QP0VA582"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihMgjx60hCHM"
      },
      "source": [
        "# PPO까지 모두 완료된 모델로 추론하기\n",
        "\n",
        "맨 위의 상수 설정 필수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcRr_cZThBqa"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from chatgpt.models.bloom import BLOOMActor\n",
        "from chatgpt.models.gpt import GPTActor\n",
        "from chatgpt.models.opt import OPTActor\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import os\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\":\n",
        "    (\n",
        "     \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
        "     ),\n",
        "    \"prompt_no_input\":\n",
        "    (\n",
        "     \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_hA3_OChEwB"
      },
      "outputs": [],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model',\n",
        "                    default='gpt2')\n",
        "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--model_path', type=str, default=None)\n",
        "parser.add_argument('--input',\n",
        "                    type=str,\n",
        "                    default='Question: How are you ? Answer:')\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args_inference = parser.parse_args([])\n",
        "\n",
        "args_inference.model = 'gpt2'\n",
        "args_inference.pretrain = SFT_MODEL_NAME\n",
        "args_inference.model_directory = PPO_MODEL_PATH\n",
        "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
        "\n",
        "# configure model, tokenizer\n",
        "actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
        "                                          padding_side=\"right\",\n",
        "                                          model_max_length=512)\n",
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "    \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "    \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
        "actor.model.load_state_dict(state_dict);\n",
        "\n",
        "actor.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTuxkw5bhEmJ"
      },
      "outputs": [],
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args_inference.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print('#' * 70)\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "list_prompt = PROMPT_LIST\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "    output = generation(input_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "chatgpt",
      "language": "python",
      "name": "chatgpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afb10a8b1b164d68949bf7dbf1f6b7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66d4fb49c90e4447a5b4a7c1861f20bd",
              "IPY_MODEL_38787e488ec9498baf13f66849b74d26",
              "IPY_MODEL_bf3d64f9ed8445f296a789d28c0e773d"
            ],
            "layout": "IPY_MODEL_9f46e8f58e2e40388dd3c2ab87f9a76d"
          }
        },
        "66d4fb49c90e4447a5b4a7c1861f20bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_654513d28bbd438c8af21a3b717de678",
            "placeholder": "​",
            "style": "IPY_MODEL_d014641320ea47b581a2b7d01964b5f6",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "38787e488ec9498baf13f66849b74d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a0ed9b3af64c7a9052ea0e37682aad",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65ba40ee84524b1e90102568cb2320f5",
            "value": 1000
          }
        },
        "bf3d64f9ed8445f296a789d28c0e773d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_433380c5b47e4b6289492d5b4ff191d2",
            "placeholder": "​",
            "style": "IPY_MODEL_f3a1da3384934e2bbd0de5c6361b908a",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 53.7kB/s]"
          }
        },
        "9f46e8f58e2e40388dd3c2ab87f9a76d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654513d28bbd438c8af21a3b717de678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d014641320ea47b581a2b7d01964b5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7a0ed9b3af64c7a9052ea0e37682aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ba40ee84524b1e90102568cb2320f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "433380c5b47e4b6289492d5b4ff191d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a1da3384934e2bbd0de5c6361b908a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2183dc035b3e49f68be75b3013e445fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f976dc34daf34b63b238cb8d85ceba5a",
              "IPY_MODEL_a98a919987c743c6844fe6b5f9deaafb",
              "IPY_MODEL_b4f2e69be88a43a293b276539fedbc7e"
            ],
            "layout": "IPY_MODEL_879062b5d9d147789d142fe09107ecf5"
          }
        },
        "f976dc34daf34b63b238cb8d85ceba5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d452703cd14bec875d30c97163031f",
            "placeholder": "​",
            "style": "IPY_MODEL_bae2ff3bff2448548a95af3b142460bb",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "a98a919987c743c6844fe6b5f9deaafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3059cbdef1564e4a8f99d269c93db85a",
            "max": 2825034,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06248e099d274a1fb2967ab38434161c",
            "value": 2825034
          }
        },
        "b4f2e69be88a43a293b276539fedbc7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b89f1869bc4481b731e302481a9fa8",
            "placeholder": "​",
            "style": "IPY_MODEL_020c7a7ef00c42fdb15bd6c2c8481c66",
            "value": " 2.83M/2.83M [00:00&lt;00:00, 18.3MB/s]"
          }
        },
        "879062b5d9d147789d142fe09107ecf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d452703cd14bec875d30c97163031f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bae2ff3bff2448548a95af3b142460bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3059cbdef1564e4a8f99d269c93db85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06248e099d274a1fb2967ab38434161c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77b89f1869bc4481b731e302481a9fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "020c7a7ef00c42fdb15bd6c2c8481c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}